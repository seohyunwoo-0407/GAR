import csv
import json
import logging
import os
from typing import Any, Callable, Dict, List, Literal, Optional, Tuple

import pytrec_eval

from financerag.common import Generator, HFDataLoader, Reranker, Retrieval
from financerag.tasks.TaskMetadata import TaskMetadata

logger = logging.getLogger(__name__)


# Adapted from https://github.com/embeddings-benchmark/mteb/blob/main/mteb/abstasks/AbsTask.py
class BaseTask:
    """
    Base class for handling tasks related to document retrieval, reranking, and generation in the finance domain.
    The class loads data, handles retrieval and reranking operations, and can generate results using a language model.

    Attributes:
        metadata (`TaskMetadata`):
            Metadata containing task-specific information, such as dataset paths and subsets.
        queries (`Optional[Dict[str, str]]`, defaults to `None`):
            A dictionary mapping query IDs to query text.
        corpus (`Optional[Dict[str, Dict[str, str]]]`, defaults to `None`):
            A dictionary mapping document IDs to a dictionary containing the document title and text.
        retrieve_results (`Optional[Dict]`, defaults to `None`):
            The results of the retrieval process.
        rerank_results (`Optional[Dict]`, defaults to `None`):
            The results of the reranking process.
        generate_results (`Optional[Dict]`, defaults to `None`):
            The results generated by the model.

    Methods:
        load_data():
            Loads the dataset (queries and corpus) into memory from the provided metadata.
        retrieve(retriever: Retrieval, top_k: Optional[int] = 100, **kwargs):
            Performs document retrieval based on the given queries and corpus.
        rerank(reranker: Reranker, results: Optional[Dict] = None, top_k: Optional[int] = 100, batch_size: Optional[int] = None, **kwargs):
            Reranks the retrieved results using the given reranker model.
        generate(model: Generator, results: Optional[Dict] = None, prepare_messages: Optional[Callable] = None, **kwargs):
            Generates results based on the highest-scoring documents from the reranked or retrieved results.
        prepare_generation_inputs(results: Dict, prepare_messages: Callable) -> Dict[str, List[dict]]:
            Prepares the input format required for generating results by the model.
        save_results(top_k: int = 10, output_dir: Optional[str] = None) -> None:
            Saves the results (retrieval, reranking, and generated) to CSV and JSONL files.
    """

    def __init__(self, metadata: TaskMetadata):
        """
        Initializes the BaseTask class with metadata for loading and processing retrieval tasks.

        Args:
            metadata (`TaskMetadata`):
                Task-specific metadata that contains dataset information and configurations.
        """
        self.metadata: TaskMetadata = metadata
        self.queries: Optional[Dict[str, str]] = None
        self.corpus: Optional[Dict[str, Dict[Literal["title", "text"], str]]] = None
        self.retrieve_results: Optional[Dict] = None
        self.rerank_results: Optional[Dict] = None
        self.generate_results: Optional[Dict] = None

        self.load_data()

    @property
    def metadata_dict(self) -> Dict[str, Any]:
        """
        Converts the task metadata into a dictionary format.

        Returns:
            `Dict[str, Any]`:
                A dictionary representation of the task metadata.
        """
        return dict(self.metadata)

    def load_data(self):
        """
        Loads the corpus and queries from the specified dataset path and subset in the metadata.

        Raises:
            `ValueError`:
                If the dataset cannot be loaded from the specified path and subset.
        """
        if (self.corpus is None) or (self.queries is None):
            dataset_path = self.metadata_dict["dataset"]["path"]
            subset = self.metadata_dict["dataset"]["subset"]

            corpus, queries = HFDataLoader(
                hf_repo=dataset_path,
                subset=subset,
                keep_in_memory=False,
            ).load()

            self.queries = {query["id"]: query["text"] for query in queries}
            self.corpus = {
                doc["id"]: {"title": doc["title"], "text": doc["text"]}
                for doc in corpus
            }

    def retrieve(
            self, retriever: Retrieval, top_k: Optional[int] = 100, **kwargs
    ) -> Dict[str, Dict[str, float]]:
        """
        Performs document retrieval using the provided retriever model.

        Args:
            retriever (`Retrieval`):
                The retrieval model to use for retrieving documents.
            top_k (`Optional[int]`, defaults to `100`):
                The number of top results to return for each query.
            **kwargs:
                Additional keyword arguments for the retriever.

        Returns:
            `Dict[str, Dict[str, float]]`:
                A dictionary where the key is the query ID and the value is another dictionary
                mapping document IDs to their retrieval scores.

        Raises:
            `TypeError`:
                If the `retriever` is not a subclass of `Retrieval`.
            `ValueError`:
                If the data (corpus or queries) is not loaded before retrieval.
        """
        if not issubclass(type(retriever), Retrieval):
            raise TypeError(f"{type(retriever)} must be a subclass of the `Retrieval` class")

        if (self.corpus is None) or (self.queries is None):
            raise ValueError("Data has not been loaded.")

        self.retrieve_results = retriever.retrieve(
            queries=self.queries, corpus=self.corpus, top_k=top_k, **kwargs
        )

        return self.retrieve_results

    def rerank(
            self,
            reranker: Reranker,
            results: Optional[Dict[str, Dict[str, float]]] = None,
            top_k: int = 100,
            batch_size: Optional[int] = None,
            **kwargs,
    ) -> Dict[str, Dict[str, float]]:
        """
        Reranks the retrieved results using the provided reranker model.

        Args:
            reranker (`Reranker`):
                The reranker model to use for reranking the retrieved results.
            results (`Optional[Dict]`, defaults to `None`):
                The initial results to rerank. If not provided, the method uses the retrieval results.
            top_k (`Optional[int]`, defaults to `100`):
                The number of top results to return after reranking.
            batch_size (`Optional[int]`, defaults to `None`):
                The batch size to use for reranking.
            **kwargs:
                Additional keyword arguments for the reranker.

        Returns:
            `Dict[str, Dict[str, float]]`:
                A dictionary where the key is the query ID and the value is another dictionary
                mapping document IDs to reranked scores.

        Raises:
            `TypeError`:
                If the `reranker` is not a subclass of `Reranker`.
            `ValueError`:
                If the data (corpus or queries) is not loaded before reranking or both `results` and `retrieve_results` are None.
        """
        if not issubclass(type(reranker), Reranker):
            raise TypeError(f"{type(reranker)} must be a subclass of the `Reranker` class")

        if (self.corpus is None) or (self.queries is None):
            raise ValueError("Data has not been loaded.")

        if results is None:
            if self.retrieve_results is not None:
                results = self.retrieve_results
            else:
                raise ValueError("Neither retrieve_results nor results can be None simultaneously.")

        self.rerank_results = reranker.rerank(
            queries=self.queries,
            corpus=self.corpus,
            results=results,
            top_k=top_k,
            batch_size=batch_size,
            **kwargs,
        )

        return self.rerank_results

    def generate(
            self,
            model: Generator,
            results: Optional[Dict] = None,
            prepare_messages: Optional[Callable] = None,
            **kwargs,
    ) -> Dict[str, str]:
        """
        Generates responses based on the highest-scoring documents from either the reranked or retrieved results.

        Args:
            model (`Generator`):
                The model used to generate responses.
            results (`Optional[Dict]`, defaults to `None`):
                The results to generate responses from. If not provided, uses reranked or retrieved results.
            prepare_messages (`Optional[Callable]`, defaults to `None`):
                A function to prepare messages for the generation model. If not provided, a default message
                preparation function is used.
            **kwargs:
                Additional keyword arguments for the generation process.

        Returns:
            `Dict[str, str]`:
                A dictionary where the key is the query ID and the value is the generated response.

        Raises:
            `TypeError`:
                If the `model` is not a subclass of `Generator`.
            `AssertionError`:
                If neither rerank_results nor retrieve_results are available for generating responses.
        """
        if not issubclass(type(model), Generator):
            raise TypeError(f"{type(model)} must be a subclass of the `Generator` class")

        if prepare_messages is None:
            logger.info(
                "No prepare_messages function provided. "
                "Using default message preparation function, which selects the highest scored document for each query."
            )

            def default_messages(
                    query: str, documents: List[Tuple[str, float]]
            ) -> List[Dict]:
                first_document = max(documents, key=lambda x: x[1])[0]
                messages = [
                    {"role": "system", "content": "You are a helpful assistant."},
                    {
                        "role": "user",
                        "content": f"Document: {first_document}"
                                   f"\nGenerate an answer to the question from the document."
                                   f"\nQuestion: {query}",
                    },
                ]
                return messages

            prepare_messages = default_messages

        if results is None:
            results = (
                self.rerank_results
                if self.rerank_results is None
                else self.retrieve_results
            )
            assert results is not None, (
                "Neither rerank_results nor retrieve_results are available. "
                "One of them must be provided."
            )

        messages_dict = self.prepare_generation_inputs(results, prepare_messages)
        self.generate_results = model.generation(messages_dict, **kwargs)

        return self.generate_results

    def prepare_generation_inputs(
            self, results, prepare_messages
    ) -> Dict[str, List[dict]]:
        """
        Prepares the input messages required for the generation model.

        Args:
            results (`Dict`):
                The results from retrieval or reranking, which are used to generate responses.
            prepare_messages (`Callable`):
                A function that prepares the messages required for the generation model.

        Returns:
            `Dict[str, List[dict]]`:
                A dictionary where the key is the query ID and the value is a list of messages (dictionaries)
                that will be passed to the generation model.

        Raises:
            `ValueError`:
                If the data (corpus or queries) is not loaded.
        """
        if (self.corpus is None) or (self.queries is None):
            raise ValueError("Data has not been loaded.")

        messages_dict: Dict[str, List[Dict[str, str]]] = {}
        logger.info("Preparing generation inputs for %d queries.", len(results))
        for query_id, result in results.items():
            query = self.queries[query_id]
            documents = [
                (self.corpus[doc_id], score) for doc_id, score in result.items()
            ]
            messages = prepare_messages(query, documents)
            messages_dict[query_id] = messages

        logger.info("Successfully prepared generation inputs for all queries.")
        return messages_dict

    def save_results(self, top_k: int = 10, output_dir: Optional[str] = None) -> None:
        """
        Saves the top retrieval or reranking, and generated results to CSV and JSONL files.

        Args:
            top_k (`int`, defaults to `10`):
                The number of top results to save for each query.
            output_dir (`Optional[str]`, defaults to `None`):
                The directory where the results should be saved. If not provided, results are not saved.

        Saves:
            - Top `top_k` retrieval or reranked results in CSV format.
            - Generated responses in JSONL format.
        """
        # If no output directory is provided, stop saving.
        if output_dir is None:
            return
        # Create the output directory if it does not exist
        output_dir = os.path.join(output_dir, self.metadata.name)
        os.makedirs(output_dir, exist_ok=True)

        logger.info(f"Output directory set to: {output_dir}")

        # Path to save the CSV file
        csv_file_path = os.path.join(output_dir, "results.csv")
        logger.info(f"Saving top {top_k} results to CSV file: {csv_file_path}")

        # Determine whether to use rerank results or retrieve results
        final_result = (
            self.rerank_results
            if self.rerank_results is not None
            else self.retrieve_results
        )

        # Process the final result if it's not None
        if final_result is not None:
            with open(csv_file_path, mode="w", newline="") as csv_file:
                writer = csv.writer(csv_file)
                # Write the header to the CSV file
                writer.writerow(["query_id", "corpus_id"])
                logger.info("Writing header ['query_id', 'corpus_id'] to CSV.")

                # For each query_id, save the top_k corpus_ids sorted by score
                for q_id, doc_scores in final_result.items():
                    # Sort doc_scores by score and select top_k documents
                    sorted_docs = sorted(
                        doc_scores.items(), key=lambda item: item[1], reverse=True
                    )[:top_k]

                    # Write the query_id and corpus_id to the CSV
                    for doc_id, _ in sorted_docs:
                        writer.writerow([q_id, doc_id])

            logger.info(f"Top {top_k} results saved successfully to {csv_file_path}")

        # Save generate_results to JSON Lines format
        if self.generate_results is not None:
            jsonl_file_path = os.path.join(output_dir, "output.jsonl")
            logger.info(f"Saving generate_results to JSONL file: {jsonl_file_path}")

            with open(jsonl_file_path, "w") as f:
                f.writelines(
                    json.dumps({"query_id": q_id, "answer": answer}) + "\n"
                    for q_id, answer in self.generate_results.items()
                )

            logger.info(f"generate_results saved successfully to {jsonl_file_path}")

    # adapted from https://github.com/beir-cellar/beir/blob/main/beir/retrieval/evaluation.py
    @staticmethod
    def evaluate(
            qrels: Dict[str, Dict[str, int]],
            results: Dict[str, Dict[str, float]],
            k_values: List[int],
            ignore_identical_ids: bool = True
    ) -> Tuple[Dict[str, float], Dict[str, float], Dict[str, float], Dict[str, float]]:

        if ignore_identical_ids:
            logger.info(
                'For evaluation, we ignore identical query and document ids (default), '
                'please explicitly set ``ignore_identical_ids=False`` to ignore this.')
            popped = []
            for qid, rels in results.items():
                for pid in list(rels):
                    if qid == pid:
                        results[qid].pop(pid)  # remove identical query-document pairs
                        popped.append(pid)

        # Filter results to only keep queries that are present in qrels
        filtered_results = {qid: rels for qid, rels in results.items() if qid in qrels}

        # Initialize dictionaries for evaluation metrics
        ndcg = {}
        _map = {}
        recall = {}
        precision = {}

        # Initialize metric values for each k in k_values
        for k in k_values:
            ndcg[f"NDCG@{k}"] = 0.0
            _map[f"MAP@{k}"] = 0.0
            recall[f"Recall@{k}"] = 0.0
            precision[f"P@{k}"] = 0.0

        # Define strings for pytrec_eval evaluation
        map_string = "map_cut." + ",".join([str(k) for k in k_values])
        ndcg_string = "ndcg_cut." + ",".join([str(k) for k in k_values])
        recall_string = "recall." + ",".join([str(k) for k in k_values])
        precision_string = "P." + ",".join([str(k) for k in k_values])

        # Perform evaluation using pytrec_eval with filtered results
        evaluator = pytrec_eval.RelevanceEvaluator(qrels,
                                                   {map_string, ndcg_string, recall_string, precision_string})
        scores = evaluator.evaluate(filtered_results)

        # Aggregate the scores for each query and each k
        for query_id in scores.keys():
            for k in k_values:
                ndcg[f"NDCG@{k}"] += scores[query_id]["ndcg_cut_" + str(k)]
                _map[f"MAP@{k}"] += scores[query_id]["map_cut_" + str(k)]
                recall[f"Recall@{k}"] += scores[query_id]["recall_" + str(k)]
                precision[f"P@{k}"] += scores[query_id]["P_" + str(k)]

        # Compute the average scores for each k
        for k in k_values:
            ndcg[f"NDCG@{k}"] = round(ndcg[f"NDCG@{k}"] / len(scores), 5)
            _map[f"MAP@{k}"] = round(_map[f"MAP@{k}"] / len(scores), 5)
            recall[f"Recall@{k}"] = round(recall[f"Recall@{k}"] / len(scores), 5)
            precision[f"P@{k}"] = round(precision[f"P@{k}"] / len(scores), 5)

        # Log the results for each metric
        for _eval in [ndcg, _map, recall, precision]:
            logger.info("\n")
            for k in _eval.keys():
                logger.info("{}: {:.4f}".format(k, _eval[k]))

        return ndcg, _map, recall, precision